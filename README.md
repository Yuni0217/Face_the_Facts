# Face the Facts

**Face the Facts** is aim to combine voice/face/text recognition and privacy preserved ML to support diagnosis and treatment of people with depressions. The purpose is to support a higher level of automation in either self-diagnosis of clinical identification of depression levels, which improves self-awareness of mental health and allows accurate depression detection at an early stage. Meanwhile, our proposed smart watch enhances treatment of depresssion in a more consistent manner.

According to [NHS](https://www.nhs.uk/mental-health/conditions/clinical-depression/symptoms/), depression can be detected through physical-, psychological-, and social-symptons. Psychological symptoms are commonly known, containing symptons like feeling hopeless and helpless, or having thoughts of harming oneself. Physical symptoms include examples like changes in appetite or weight, or changes of sleeping habits. Social symptoms are seen in less social activities than usual. However, existing diagnosis of depression relies on subjective answers to mental health questionnaires, which is based on the patients' memory of the past weeks or months. Even if a patient is diagnosed and gets access to depression care, consistent and customised treatment is challenging to provide. 

Here, we propose a smart watch, **Face the Facts**, with an attempt to tackle these challenges in depression detection and treatment. 

## System Design
<p align="center">
<img src="https://github.com/Yuni0217/Face_the_Facts/blob/main/others/FacetheFacts.png" alt="System" width="550px">
</p>

**Step 1: Data Collection** 

**Step 2: Privacy-Preserved Machine-Learning Training/Testing** 

**Step 3: Fuzzy-Logic Based Computing** 

## Data
* Facial data: get through webcam embedded in our smart watch, and generates facial emotion features (to diagnose psychological symptoms).
* Speech data: get through microphone embedded in our smart watch, and generates voice emotion features (to diagnose psychological and social symptoms).
* Text communication data: get through connected socialisation APP in a phone/pad/PC, and generates textual emotion features (to diagnose psychological and social symptoms). 
* Medical data: get through sensors embedded in our smart watch and/or connected APP, and generates biomedical fectures (to diagnose physical symptoms).

## Reference
* [CMU-MOSEI](https://github.com/A2Zadeh/CMU-MultimodalSDK) for voice emotion detection.
* [Toronto emotional speech set (TESS)](https://tspace.library.utoronto.ca/handle/1807/24487) for voice emotion detection.
